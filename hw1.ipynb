{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Regression\n",
    "\n",
    "### Make sure that you read and fully understand all the guidelines listed below before you proceed with the exercise.\n",
    "\n",
    "* HW assignments are a significant part of the learning experience in this course and contribute 50% to your final grade. So, make sure to devote the appropriate time to them.\n",
    "* **Sharing solutions with someone who is not your submitting partner is strictly prohibited**. This includes reading someone else's code or sharing your code / posting it somewhere.\n",
    "* Appeals regarding submissions that do not follow the guidelines will not be accepted. \n",
    "\n",
    "\n",
    "### Guidelines for Programming Exercises:\n",
    "\n",
    "* Complete the required functions in `hw1.py`. Any modifications to this notebook will not be tested by our automated tests.\n",
    "* Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise can take several minutes when implemented efficiently, but will take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "* You are responsible for the correctness of your code. You can add tests to this jupyter notebook to validate your solution. The contents of this jupyter notebook will not be graded or checked.\n",
    "* You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/), numpy and pandas only. **Do not import anything else.**\n",
    "* Use `numpy` version 1.15.4 or higher.\n",
    "* Your code must run without errors. Code that cannot run will not be graded.\n",
    "* Your code will be tested using automated scripts. So, failure to follow the instructions may lead to test failure, which might significantly affect your grade. \n",
    "\n",
    "\n",
    "### Guidlines for Theoretical Exercises\n",
    "* Your solution should be written or typed and submitted in a separate file `hw1.pdf`.\n",
    "* If you scan a handwritten solution, make sure that your handwriting is legible and the scan quality is good.\n",
    "* You are expected to solve the questions analytically and provide a step-by-step solution. \n",
    "* It is okay and often recommended to use python to carry out the computations. \n",
    "* You may use the lecture slides and previous homework assignments as references, unless explicitly asked to prove a result from class. \n",
    "\n",
    "### Submission Guidelines:\n",
    "* Submit your solutiuon in a zip file that contains: \n",
    "  - The `hw1.py` script with your solution to the progamming exercise\n",
    "  - This notebook with your added tests (this is not checked or graded)\n",
    "  - The `hw1.pdf` file with your solution to the theoretical exercises.\n",
    "  \n",
    "* The name of the zip file should contain your ID(s). For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone.\n",
    "* Please use **only a zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** YOUR ID HERE ***\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theoretical Exercises (16 points)\n",
    "\n",
    "We have the following data:\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "x & y \\\\\n",
    "\\hline\n",
    "-1 & -1 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & 2 \\\\\n",
    "2 & 3 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "1. We would like to fit a linear regression model to this data for the purpose of predicting future values of $y$ from $x$.\n",
    "    - Write the data matrix $X$ for this regression. Make sure to include the bias term.\n",
    "    - Write the pseudo inverse $X^\\dagger$ of $X$.\n",
    "    - Use $X^\\dagger$ to find the vector $\\theta^* \\in \\mathbb R^2$ that minimizes the sum of squares loss:\n",
    "    $$\n",
    "    J(\\theta) = \\sum_{i=1}^n \\left( \\theta^\\top (1,x^{(i)}) - y^{(i)}  \\right)^2\n",
    "    $$\n",
    "    - Compute the minimum loss $J(\\theta^*)$.\n",
    "2. Confirm that this is the minimum loss using calculus.\n",
    "    - Exprss the loss in the form\n",
    "    $   J(\\theta) = A \\theta_0^2 + B \\theta_1 \\theta_0 + C \\theta_1^2 + D \\theta_0 + E \\theta_1 + F$,\n",
    "      for some $A$, $B$, $C$, $D$, $E$, and $F$ that depend on $x$ and $y$.\n",
    "    - Find an expression for the gradient $\\nabla J(\\theta) \\in \\mathbb R^2$ for aritrary $\\theta \\in \\mathbb R^2$.  \n",
    "    - Show that $\\nabla J(\\theta^*) = 0$.\n",
    "3. Consider the prediction of $y$ at a test point $x=1.5$. \n",
    "    - What is the predicted value of $y$ at this point based on linear regression with $\\theta^*$?\n",
    "    - What is the predicted value of $y$ at this point based on K-NN with $K=2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Coding Assignment (84 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:02.881205Z",
     "start_time": "2024-04-09T10:27:02.827680Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for scientific computing\n",
    "import pandas as pd # used for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\n",
    "np.random.seed(42) \n",
    "\n",
    "# make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Data Loading, Exploration, and Preprocessing (10 points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County. The dataset contains 5,000 observations with 18 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.054729Z",
     "start_time": "2024-04-09T10:27:02.898466Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in data-oriented projects is to explore and characterize general properties of the data. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.063657Z",
     "start_time": "2024-04-09T10:27:04.060650Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.064443Z",
     "start_time": "2024-04-09T10:27:04.064347Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will start with simple linear regression with one feature by extracting the target column (`price`) and the first feature (`sqft_living`) from the dataset. We use pandas and select both columns as separate variables and transform them into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.067277Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Normalization\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use [Standardization](https://en.wikipedia.org/wiki/Feature_scaling) for the fearures (`X`) and the true labels (`y`). \n",
    "\n",
    "Please note that normalization is not always necessary or a good idea. For example, normalization may not be a good idea for datasets with outliers.\n",
    "\n",
    "Implement Standartization transformation:\n",
    "$$\n",
    "x'_i = \\frac{x_i - \\bar{x}}{\\sigma},\\qquad \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$$\n",
    "(here $x = (x_1, x_2, \\dots, x_n)$ is a sequence of values)\n",
    "This transformation is applied to each feature independently, as well as to the target variable.\n",
    "\n",
    "Your implementation should not contain loops.\n",
    "\n",
    "---\n",
    "Complete the function `preprocess` in `hw1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.070751Z",
     "start_time": "2024-04-09T10:27:04.069564Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import preprocess\n",
    "\n",
    "X, y = preprocess(X, y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "We partition the data into two random parts: \n",
    "1. The training dataset will contain 80% of the data and will be used for model training.\n",
    "2. The validation dataset will contain the remaining 20% of the data and will be used for model evaluation.\n",
    "\n",
    "Testing a model on a dataset that was not used for training provides reliable evaluation of our model's expected performance on unseen instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.072211Z",
     "start_time": "2024-04-09T10:27:04.072095Z"
    }
   },
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train], X[idx_val]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Visualization\n",
    "Another useful concept is data visualization. The code below plots the target value (`price`) against the first feature (`sqft_living`). Since we are currently considering just one feature, we can visualize its relation to the target value using a two-dimensional scatterplot. Once we consider more than two features, visualization becomes much more complicated. We will be using `matplotlib` for all data visualization purposes since it offers a wide range of visualization tools and is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.075135Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_train, y_train, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Bias/Intercept Consideration\n",
    "\n",
    "Recall that linear regression contains a bias/intercept parameter, $\\theta_0$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "Thus, to solve linear regression problems, we need to add an \"all-1\" column as the 0th column of the feature matrix. Do this for both the training and validation data.\n",
    "\n",
    "---\n",
    "Complete the function `apply_bias_trick` in `hw1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.077336Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import apply_bias_trick\n",
    "\n",
    "X_train = apply_bias_trick(X_train)\n",
    "X_val = apply_bias_trick(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Loss Function (5 points)\n",
    "Our task is to find the linear function of the first feature (`sqft_living`) that \"best explains\" the target (or response) variable in our dataset (`price`). Since we are currently considering only one feature, the regression formula is:\n",
    "$$\n",
    "\\hat{y} = \\theta^\\top x = \\theta_0 + \\theta_1 x_1.\n",
    "$$\n",
    "The parameter vector of our model is $\\theta = (\\theta_0, \\theta_1)$.\n",
    "\n",
    "The model that \"best explains\" the target variable in the training data minimizes the loss function $J$:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n}(\\theta^\\top x^{(i)}-y^{(i)})^2.\n",
    "$$\n",
    "This loss function is called the *mean squared error* and it is obtained by scaling the *sum of squared errors* loss shown in the lecture by a (constant) factor of $\\frac{1}{2n}$. Therefore, the minimizers of these two loss functions are identical. Scaling allows us to compare losses across datasets of different sizes.\n",
    "\n",
    "---\n",
    "Complete the function `compute_loss` in `hw1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.079579Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import compute_loss\n",
    "theta = np.array([-1, 2])\n",
    "J = compute_loss(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent (15 points)\n",
    "\n",
    "We now wish to use *gradient descent* to find the $\\theta$ that minimizes the loss function $J(\\theta)$. We start with an initial guess for $\\theta$, and then update it sequentially as follows:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta),\n",
    "$$\n",
    "where $\\eta$ is the *learning rate*. Note that this is a vector update:\n",
    "$$\n",
    "\\theta_j \\leftarrow \\theta_j - \\eta \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1),\\quad j=0,1.\n",
    "$$\n",
    "\n",
    "In linear regresion, the gradient is given by the following formula:\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (\\theta^\\top x^{(i)}-y^{(i)})x^{(i)}.\n",
    "$$\n",
    "Specifically, when considering a single feaute, we get:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) &= \\frac{1}{n} \\sum_{i=1}^n (\\theta_0 + \\theta_1 x_1^{(i)} - y^{(i)}), \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) &= \\frac{1}{n} \\sum_{i=1}^n (\\theta_0 + \\theta_1 x_1^{(i)} - y^{(i)})x_1^{(i)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "Complete the function `gradient_descent` in `hw1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.082170Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import gradient_descent\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "eta = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, eta, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can monitor the learning process by tracing the loss as training progresses. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help you find problems with your code. Notice that we are using a logarithmic scale for the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.085265Z",
     "start_time": "2024-04-09T10:27:04.085138Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4. Analytical Solution to Linear Regression (10 points)\n",
    "\n",
    "The pseudo inverse method is a direct approach to finding the best-fitting parameters of the linear model. In your implementation, **do not use `np.linalg.pinv`**. Instead, use only direct matrix multiplication as you saw in class (you can calculate the inverse of a matrix using `np.linalg.inv`).\n",
    "\n",
    "---\n",
    "Complete the function `compute_pinv` in `hw1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:04.203177Z",
     "start_time": "2024-04-09T10:27:04.088316Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import compute_pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.091214Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = compute_pinv(X_train ,y_train)\n",
    "J_pinv = compute_loss(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss of the $\\theta$ calculated using the psuedo-inverse method to our graph. This provides us with another sanity check, since the loss obtained by gradient descent should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.095590Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(J_history)), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning The Learning rate (5 points)\n",
    "\n",
    "The learning rate is another factor that determines the performance of optimization algorithms. It may affect performance in several ways:\n",
    "1. If the learning rate is too high, the algorithm may diverge (see example in the lecture).\n",
    "2. If the learning rate is too low, the algorithm may converge very slowly and bloat the runtime.\n",
    "3. In non-convex optimization problems like neural network optimization (not in this course), the learning rate may not only change the speed of convergence but also the convergence point and thus the quality of the learned parameters.\n",
    "\n",
    "In the case of linear regression, unless the learning rate is very high and leads to divergence, it should only affect the convergence rate and not the convergence point. \n",
    "\n",
    "We will check several learning rates and choose the ones the attain minimum loss after a fixed number of iterations. \n",
    "\n",
    "Complete the function `find_best_learning_rate`. Use the training dataset to learn the parameters ($\\theta$), and use the **validation dataset** to compute the loss associated with these parameters.\n",
    "\n",
    "---\n",
    "Complete the function `find_best_learning_rate` in `hw1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 6. Adding A Halting Condition to The Gradient Descent (5 points)\n",
    "Earlier you implemented a function `gradient_descent` that performs a fixed number of iterations. We can use a better approach for the implementation of `gradient_descent`. Instead of performing a constant number of iterations, we wish to stop when the improvement of the loss btween two consecutive iterations is smaller than $\\epsilon$=`1e-8`. \n",
    "\n",
    "---\n",
    "Complete the function `gradient_descent_stop_condition` in `hw1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the remaining parts of this assignment, use the function `gradient_descent_stop_condition` with the best learning rate you computed above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.100113Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import gradient_descent_stop_condition, find_best_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.103913Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "eta_dict = find_best_learning_rate(X_train, y_train, X_val, y_val, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can now obtain the best learning rate from the dictionary `eta_dict` in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.106620Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "best_learning_rate = min(eta_dict, key=eta_dict.get)\n",
    "print(best_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code gets the best three learning rates you just calculated and provides a graph with three lines indicating the training loss as a function of iterations. Use it as a test for your implementation. You can change this code as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.109987Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "top_3_etas = sorted([(value, key) for key, value in eta_dict.items()], reverse=False)[:3]\n",
    "top_3_etas = [x[1] for x in top_3_etas]\n",
    "\n",
    "histories = []\n",
    "for eta in top_3_etas:\n",
    "    params = np.random.random(size=2)\n",
    "    _, J_history = gradient_descent(X_train ,y_train, params, eta, num_iters=10000)\n",
    "    histories.append(J_history)\n",
    "\n",
    "for i, (eta, color) in enumerate(zip(top_3_etas, ['b','g','r'])):\n",
    "    plt.plot(np.arange(10000), histories[i], color, label='eta='+str(eta))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 0.005)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The function below plots the regression lines of the models you obtained via gradient descent and the pseudoinverse method. Use this to check your solution, as both models should exhibit similar trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.112034Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_train[:,1], y_train, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta), 'o')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta_pinv), '-')\n",
    "\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 7. Generalizing to Multiple Features (10 points)\n",
    "\n",
    "Most datasets have more than one feature, and some might have thousands of features. In those cases, we use a multivariate linear regression model. The regression equation is similar to that of simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta^T x = \\theta_0 + \\theta_1 x_1 + ... + \\theta_p x_p\n",
    "$$\n",
    "\n",
    "If you wrote proper vectorized code, this part should be trivial and work without changes. If this is not the case, you should go back and edit your functions such that they support both multivariate and single variable regression. **Make sure to allow for an arbitrary number of features and your code should not explicitly check the dimensionality of the input**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.113998Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant so we will go ahead and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.116411Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the same `preprocess` function you implemented previously. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous parts still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.119199Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.121004Z"
    }
   },
   "outputs": [],
   "source": [
    "# training and validation split \n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Using 3D visualization, we can still observe trends in the data. Visualizing additional dimensions requires advanced techniques we will learn later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.122177Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "# Create 3D axes \n",
    "ax = fig.add_subplot(111, projection='3d')  \n",
    "\n",
    "# Plot data\n",
    "xx = X_train[:, 1][:1000]\n",
    "yy = X_train[:, 2][:1000]\n",
    "zz = y_train[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Analysis\n",
    "\n",
    "Make sure the functions `apply_bias_trick`, `compute_loss`, `gradient_descent`, `gradient_descent_stop_condition` and `compute_pinv` work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the simple regression of Part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.123215Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# bias trick\n",
    "X_train = apply_bias_trick(X_train)\n",
    "X_val = apply_bias_trick(X_val)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.124726Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# calculating the loss\n",
    "shape = X_train.shape[1]\n",
    "theta = np.ones(shape)\n",
    "J = compute_loss(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.127743Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# running the efficient version of gradient descent\n",
    "np.random.seed(42)\n",
    "shape = X_train.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "theta, J_history = gradient_descent_stop_condition(X_train ,y_train, theta, best_learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.128749Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# calculating the pseudoinverse\n",
    "theta_pinv = compute_pinv(X_train ,y_train)\n",
    "J_pinv = compute_loss(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the progression of the gradient descent. Use this to validate your code. Notice we use logarithmic scale for the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.130571Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(J_history)), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 8. Forward feature selection (12 points)\n",
    "\n",
    "Adding features to a regression model makes it more complicated and potentially more powerful, but it does not necessarily improve its performance. Forward feature selection is a greedy, iterative algorithm used to select the most informative features for a predictive model. This algorithm iteratively adds the feature that obtains the best prediction of the target value in the validation data. \n",
    "\n",
    "Implement forward feature selection using the following guidelines: \n",
    "1. Start with an empty set of model features $M\\leftarrow\\emptyset$.\n",
    "1. For each feature $j\\notin M$, do the following:\n",
    "    - Use the training data to train a linear regression model using the features in $M\\cup\\{j\\}$\n",
    "    - Evaluate this model's performance by calculating its loss on the validation data.\n",
    "1. Choose the feature, $j^*$, which provides the best model performance above and add it to the set of model features: $M\\leftarrow M\\cup\\{j^*\\}$.\n",
    "1. Repeat steps 2-3 until you have five features (not including the bias parameter).\n",
    "\n",
    "---\n",
    "Open `hw1.py` and complete the function `forward_feature_selection`.\n",
    "\n",
    "**Note that you should use the inputs as provided in the code below. Do not forget to add the bias parameter inside `forward_feature_selection`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.132099Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "feature_names = df.drop(columns=['price', 'id', 'date']).columns.values\n",
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values\n",
    "\n",
    "# preprocessing\n",
    "X, y = preprocess(X, y)\n",
    "\n",
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.133547Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import forward_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.136532Z"
    }
   },
   "outputs": [],
   "source": [
    "ffs = forward_feature_selection(X_train, y_train, X_val, y_val, best_learning_rate, iterations)\n",
    "for feature in ffs:\n",
    "    print(feature_names[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Polynomial regression (12 points)\n",
    "\n",
    "Now, we will try to model the target value using a polynomial of degree 2 of the features. Recall that this can be done by applying the algorithms for linear regression on transformed input.\n",
    "\n",
    "### Transform The Data\n",
    "\n",
    "Implement a function that takes a data matrix and creates the appropriate feature matrix for solving the degree-2 polynomial regression. The function should take a pandas DataFrame as input and should return a new DataFrame with all relevant composite features. If the input DataFrame has $p$ features, then the returned DataFrame should have $2p+\\frac{p(p-1)}{2}$ composite features: all original features, all original features squared, and the product of all feature pairs. The names of composite features in the returned DataFrame should reflect their meaning. Examples: `sqft_lot`, `yr_built^2`, `bedrooms*bathrooms`.\n",
    "\n",
    "Open `hw1.py` and complete the function `create_square_features`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.138300Z"
    }
   },
   "outputs": [],
   "source": [
    "from hw1 import create_square_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.139548Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "y = df['price'].values\n",
    "df = df.drop(columns=['price', 'id', 'date'])\n",
    "df = df.astype('float64')\n",
    "df_poly = create_square_features(df)\n",
    "X = df_poly.values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.140909Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.142803Z"
    }
   },
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.144377Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.145777Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = df_poly.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solve The Regression Problem\n",
    "\n",
    "After you obtain the polynomial dataframe, use forward feature selection to find (and print) the five best composite features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.147012Z"
    }
   },
   "outputs": [],
   "source": [
    "ffs = forward_feature_selection(X_train, y_train, X_val, y_val, best_learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-09T10:27:04.148064Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature in ffs:\n",
    "    print(feature_names[feature])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
